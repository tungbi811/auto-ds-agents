{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18cef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from multi_agents import BusinessAnalyst\n",
    "from autogen import ConversableAgent, AssistantAgent, LLMConfig, UserProxyAgent, GroupChatManager, GroupChat\n",
    "from autogen.coding.jupyter import LocalJupyterServer, JupyterCodeExecutor\n",
    "from utils.utils import request_clarification\n",
    "\n",
    "def custom_speaker_selection_func(last_speaker: ConversableAgent, group_chat: GroupChat):\n",
    "    if len(group_chat.messages) == 1:\n",
    "        return group_chat.agent_by_name(\"Data_Explorer\")\n",
    "\n",
    "    last_message = group_chat.messages[-1][\"content\"]\n",
    "    role = group_chat.messages[-1][\"role\"]\n",
    "    if role == \"tool\":\n",
    "        return group_chat.agent_by_name(\"User\")\n",
    "    if last_speaker.name == \"Data_Explorer\":\n",
    "        if \"python\" in last_message:\n",
    "            return group_chat.agent_by_name(\"CodeExecutor\")\n",
    "        else:\n",
    "            return group_chat.agent_by_name(\"Project_Manager\")\n",
    "    elif last_speaker.name == 'CodeExecutor' or last_speaker.name == 'User':\n",
    "        last_second_speaker_name = group_chat.messages[-2][\"name\"]\n",
    "        return group_chat.agent_by_name(last_second_speaker_name)\n",
    "    return group_chat.agent_by_name(\"Project_Manager\")\n",
    "\n",
    "llm_config = LLMConfig.from_json(path=\"configs/llm_config.json\")\n",
    "\n",
    "output_dir = Path(\"./artifacts\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "server = LocalJupyterServer(log_file='./logs/jupyter_gateway.log')\n",
    "executor = JupyterCodeExecutor(server, output_dir=output_dir)\n",
    "\n",
    "code_executor = ConversableAgent(\n",
    "    name=\"CodeExecutor\",\n",
    "    llm_config=False,               # stays tool-only / non-reasoning\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config={\"executor\": executor},  # will execute fenced ```python blocks\n",
    ")\n",
    "\n",
    "data_explorer = AssistantAgent(\n",
    "    name=\"Data_Explorer\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    system_message=\"\"\"You are the data explorer. Given a dataset and a task, please write code to explore and understand the properties of the dataset.\n",
    "        For example, you can:\n",
    "        - get the shape of the dataset\n",
    "        - get the first several rows of the dataset\n",
    "        - get the information of the dataset use `df.info()` or `df.describe()`\n",
    "        - plot the plots as needed (i.e. histogram, distribution)\n",
    "        - check the missing values\n",
    "        Only perform necessary data exploration steps.\n",
    "        When you need to execute Python (load CSVs, transform data, plot), write a complete Python cell fenced with ```python ...``` so the CodeExecutor can run it, \n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "project_manager = AssistantAgent(\n",
    "    name=\"Project_Manager\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    system_message=\"\"\"\n",
    "        You are the project manager. Your role is to answer other agets' questions and evaluate the results.\n",
    "        if you are not sure about something, use the request_clarification tool to ask the user for more specifics\n",
    "    \"\"\",\n",
    "    functions = [request_clarification]\n",
    ")\n",
    "\n",
    "user = UserProxyAgent(name=\"User\",code_execution_config={\"use_docker\": False})\n",
    "\n",
    "task_prompt = \"\"\"Explore the dataset data/data_science_salaries/data_science_salaries.csv and provide insights.\"\"\"\n",
    "\n",
    "pattern = GroupChat(\n",
    "    agents=[user, data_explorer,code_executor, project_manager],\n",
    "    speaker_selection_method=custom_speaker_selection_func,\n",
    "    max_round=10\n",
    ")\n",
    "\n",
    "# group_chat_manager = GroupChatManager(pattern, llm_config=llm_config)\n",
    "\n",
    "# result = user.initiate_chat(group_chat_manager, message=task_prompt)\n",
    "\n",
    "bis_analyst = BusinessAnalyst()\n",
    "user.initiate_chat(bis_analyst, message=\"Analyst for me this data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4647b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Optional, List, Dict\n",
    "from pydantic import BaseModel, Field\n",
    "from autogen import (\n",
    "    ConversableAgent,\n",
    "    UserProxyAgent,\n",
    "    LLMConfig,\n",
    ")\n",
    "from autogen.agentchat import initiate_group_chat\n",
    "from autogen.agentchat.group.patterns import DefaultPattern\n",
    "from autogen.agentchat.group import (\n",
    "    AgentTarget,\n",
    "    AgentNameTarget,\n",
    "    RevertToUserTarget,\n",
    "    ContextVariables,\n",
    ")\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import os\n",
    "\n",
    "# Setup LLM configuration (using a placeholder)\n",
    "# In a real scenario, you would configure this with your actual LLM provider\n",
    "llm_config = LLMConfig(api_type=\"openai\", model=\"gpt-4\", cache_seed=None)\n",
    "\n",
    "# --- Pydantic Models for Data Science Workflow ---\n",
    "\n",
    "class DataIngestionResult(BaseModel):\n",
    "    \"\"\"Result of the data ingestion stage.\"\"\"\n",
    "    data_loaded: bool = Field(..., description=\"Indicates if the data was loaded successfully.\")\n",
    "    dataset_name: str = Field(..., description=\"Name of the dataset loaded.\")\n",
    "    num_rows: int = Field(..., description=\"Number of rows in the loaded dataset.\")\n",
    "    num_columns: int = Field(..., description=\"Number of columns in the loaded dataset.\")\n",
    "    error_message: Optional[str] = Field(None, description=\"Error message if ingestion failed.\")\n",
    "\n",
    "class DataCleaningResult(BaseModel):\n",
    "    \"\"\"Result of the data cleaning stage.\"\"\"\n",
    "    cleaning_successful: bool = Field(..., description=\"Indicates if data cleaning was successful.\")\n",
    "    rows_after_cleaning: int = Field(..., description=\"Number of rows remaining after cleaning.\")\n",
    "    columns_after_cleaning: int = Field(..., description=\"Number of columns remaining after cleaning.\")\n",
    "    missing_values_handled: bool = Field(..., description=\"Indicates if missing values were handled.\")\n",
    "    duplicates_removed: bool = Field(..., description=\"Indicates if duplicate rows were removed.\")\n",
    "    error_message: Optional[str] = Field(None, description=\"Error message if cleaning failed.\")\n",
    "\n",
    "class FeatureEngineeringResult(BaseModel):\n",
    "    \"\"\"Result of the feature engineering stage.\"\"\"\n",
    "    features_created: bool = Field(..., description=\"Indicates if new features were successfully created.\")\n",
    "    feature_list: List[str] = Field(..., description=\"List of features for model training.\")\n",
    "    target_variable: str = Field(..., description=\"The target variable for the model.\")\n",
    "    error_message: Optional[str] = Field(None, description=\"Error message if feature engineering failed.\")\n",
    "\n",
    "class ModelTrainingResult(BaseModel):\n",
    "    \"\"\"Result of the model training stage.\"\"\"\n",
    "    model_trained: bool = Field(..., description=\"Indicates if the model was trained successfully.\")\n",
    "    model_type: str = Field(..., description=\"Type of model that was trained (e.g., RandomForestClassifier).\")\n",
    "    training_accuracy: float = Field(..., description=\"Accuracy of the model on the training set.\")\n",
    "    error_message: Optional[str] = Field(None, description=\"Error message if training failed.\")\n",
    "\n",
    "class ModelEvaluationResult(BaseModel):\n",
    "    \"\"\"Result of the model evaluation stage.\"\"\"\n",
    "    evaluation_complete: bool = Field(..., description=\"Indicates if the model evaluation is complete.\")\n",
    "    test_accuracy: float = Field(..., description=\"Accuracy of the model on the test set.\")\n",
    "    classification_report: Dict = Field(..., description=\"Classification report including precision, recall, f1-score.\")\n",
    "    model_performance_summary: str = Field(..., description=\"A summary of the model's performance.\")\n",
    "\n",
    "\n",
    "# --- Shared Context for the Pipeline ---\n",
    "\n",
    "shared_context = ContextVariables(\n",
    "    data={\n",
    "        \"pipeline_started\": False,\n",
    "        \"data_ingestion_completed\": False,\n",
    "        \"data_cleaning_completed\": False,\n",
    "        \"feature_engineering_completed\": False,\n",
    "        \"model_training_completed\": False,\n",
    "        \"model_evaluation_completed\": False,\n",
    "        \"has_error\": False,\n",
    "        \"error_message\": \"\",\n",
    "        \"error_stage\": \"\",\n",
    "        \"raw_dataframe\": None,\n",
    "        \"cleaned_dataframe\": None,\n",
    "        \"features\": None,\n",
    "        \"target\": None,\n",
    "        \"model\": None\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# --- Pipeline Stage Functions (Tools for Agents) ---\n",
    "\n",
    "def ingest_data(file_path: str, context_variables: ContextVariables):\n",
    "    \"\"\"Loads the initial dataset from a given file path into the pipeline.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        context_variables[\"raw_dataframe\"] = df.to_json()\n",
    "        context_variables[\"data_ingestion_completed\"] = True\n",
    "        return DataIngestionResult(\n",
    "            data_loaded=True,\n",
    "            dataset_name=file_path,\n",
    "            num_rows=len(df),\n",
    "            num_columns=len(df.columns)\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        error_msg = f\"Error: The file was not found at the path: {file_path}\"\n",
    "        context_variables[\"has_error\"] = True\n",
    "        context_variables[\"error_stage\"] = \"data_ingestion\"\n",
    "        context_variables[\"error_message\"] = error_msg\n",
    "        return DataIngestionResult(data_loaded=False, dataset_name=file_path, num_rows=0, num_columns=0, error_message=error_msg)\n",
    "    except Exception as e:\n",
    "        context_variables[\"has_error\"] = True\n",
    "        context_variables[\"error_stage\"] = \"data_ingestion\"\n",
    "        context_variables[\"error_message\"] = str(e)\n",
    "        return DataIngestionResult(data_loaded=False, dataset_name=file_path, num_rows=0, num_columns=0, error_message=str(e))\n",
    "\n",
    "\n",
    "def clean_data(context_variables: ContextVariables):\n",
    "    \"\"\"Cleans the loaded data by handling missing values and duplicates.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_json(context_variables[\"raw_dataframe\"])\n",
    "        \n",
    "        # Handle missing values\n",
    "        if 'age' in df.columns:\n",
    "            df['age'].fillna(df['age'].median(), inplace=True)\n",
    "        missing_handled = True\n",
    "        \n",
    "        # Remove duplicates\n",
    "        initial_rows = len(df)\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        duplicates_removed = initial_rows > len(df)\n",
    "\n",
    "        context_variables[\"cleaned_dataframe\"] = df.to_json()\n",
    "        context_variables[\"data_cleaning_completed\"] = True\n",
    "        return DataCleaningResult(\n",
    "            cleaning_successful=True,\n",
    "            rows_after_cleaning=len(df),\n",
    "            columns_after_cleaning=len(df.columns),\n",
    "            missing_values_handled=missing_handled,\n",
    "            duplicates_removed=duplicates_removed\n",
    "        )\n",
    "    except Exception as e:\n",
    "        context_variables[\"has_error\"] = True\n",
    "        context_variables[\"error_stage\"] = \"data_cleaning\"\n",
    "        context_variables[\"error_message\"] = str(e)\n",
    "        return DataCleaningResult(cleaning_successful=False, rows_after_cleaning=0, columns_after_cleaning=0, missing_values_handled=False, duplicates_removed=False, error_message=str(e))\n",
    "\n",
    "def engineer_features(context_variables: ContextVariables):\n",
    "    \"\"\"Creates new features for the model.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_json(context_variables[\"cleaned_dataframe\"])\n",
    "        \n",
    "        # One-hot encode categorical features\n",
    "        if 'department' in df.columns:\n",
    "            df = pd.get_dummies(df, columns=['department'], drop_first=True)\n",
    "        \n",
    "        target_variable = 'has_purchased'\n",
    "        if target_variable not in df.columns:\n",
    "            raise ValueError(f\"Target variable '{target_variable}' not found in the dataframe.\")\n",
    "\n",
    "        feature_list = [col for col in df.columns if col != target_variable]\n",
    "        \n",
    "        context_variables[\"features\"] = df[feature_list].to_json()\n",
    "        context_variables[\"target\"] = df[target_variable].to_json()\n",
    "        context_variables[\"feature_engineering_completed\"] = True\n",
    "        \n",
    "        return FeatureEngineeringResult(\n",
    "            features_created=True,\n",
    "            feature_list=feature_list,\n",
    "            target_variable=target_variable\n",
    "        )\n",
    "    except Exception as e:\n",
    "        context_variables[\"has_error\"] = True\n",
    "        context_variables[\"error_stage\"] = \"feature_engineering\"\n",
    "        context_variables[\"error_message\"] = str(e)\n",
    "        return FeatureEngineeringResult(features_created=False, feature_list=[], target_variable=\"\", error_message=str(e))\n",
    "\n",
    "\n",
    "def train_model(context_variables: ContextVariables):\n",
    "    \"\"\"Trains a machine learning model.\"\"\"\n",
    "    try:\n",
    "        X = pd.read_json(context_variables[\"features\"])\n",
    "        y = pd.read_json(context_variables[\"target\"], typ='series')\n",
    "\n",
    "        X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        model = RandomForestClassifier(random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Save model - in a real scenario, you'd serialize this properly\n",
    "        context_variables[\"model\"] = model\n",
    "        context_variables[\"model_training_completed\"] = True\n",
    "        \n",
    "        train_accuracy = accuracy_score(y_train, model.predict(X_train))\n",
    "        \n",
    "        return ModelTrainingResult(\n",
    "            model_trained=True,\n",
    "            model_type=\"RandomForestClassifier\",\n",
    "            training_accuracy=train_accuracy\n",
    "        )\n",
    "    except Exception as e:\n",
    "        context_variables[\"has_error\"] = True\n",
    "        context_variables[\"error_stage\"] = \"model_training\"\n",
    "        context_variables[\"error_message\"] = str(e)\n",
    "        return ModelTrainingResult(model_trained=False, model_type=\"\", training_accuracy=0.0, error_message=str(e))\n",
    "\n",
    "def evaluate_model(context_variables: ContextVariables):\n",
    "    \"\"\"Evaluates the trained model on the test set.\"\"\"\n",
    "    try:\n",
    "        X = pd.read_json(context_variables[\"features\"])\n",
    "        y = pd.read_json(context_variables[\"target\"], typ='series')\n",
    "        \n",
    "        _, X_test, _, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        model = context_variables[\"model\"]\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        test_accuracy = accuracy_score(y_test, predictions)\n",
    "        report = classification_report(y_test, predictions, output_dict=True, zero_division=0)\n",
    "        \n",
    "        summary = f\"Model achieved a test accuracy of {test_accuracy:.2f}.\"\n",
    "        \n",
    "        context_variables[\"model_evaluation_completed\"] = True\n",
    "        \n",
    "        return ModelEvaluationResult(\n",
    "            evaluation_complete=True,\n",
    "            test_accuracy=test_accuracy,\n",
    "            classification_report=report,\n",
    "            model_performance_summary=summary\n",
    "        )\n",
    "    except Exception as e:\n",
    "        context_variables[\"has_error\"] = True\n",
    "        context_variables[\"error_stage\"] = \"model_evaluation\"\n",
    "        context_variables[\"error_message\"] = str(e)\n",
    "        return ModelEvaluationResult(evaluation_complete=False, test_accuracy=0.0, classification_report={}, model_performance_summary=str(e))\n",
    "\n",
    "# --- Conversable Agents for Each Pipeline Stage ---\n",
    "\n",
    "with llm_config:\n",
    "    entry_agent = ConversableAgent(\n",
    "        name=\"Data_Ingestion_Agent\",\n",
    "        system_message=\"Your task is to start the data science pipeline by loading the data from a user-provided file path. Call the ingest_data tool with the file path.\",\n",
    "        llm_config=llm_config,\n",
    "        functions=[ingest_data]\n",
    "    )\n",
    "\n",
    "    cleaning_agent = ConversableAgent(\n",
    "        name=\"Data_Cleaning_Agent\",\n",
    "        system_message=\"Your task is to clean the dataset. Handle missing values and remove duplicates by calling the clean_data tool.\",\n",
    "        llm_config=llm_config,\n",
    "        functions=[clean_data]\n",
    "    )\n",
    "\n",
    "    feature_agent = ConversableAgent(\n",
    "        name=\"Feature_Engineering_Agent\",\n",
    "        system_message=\"Your task is to perform feature engineering. Create new features from the cleaned data by calling the engineer_features tool.\",\n",
    "        llm_config=llm_config,\n",
    "        functions=[engineer_features]\n",
    "    )\n",
    "\n",
    "    training_agent = ConversableAgent(\n",
    "        name=\"Model_Training_Agent\",\n",
    "        system_message=\"Your task is to train a machine learning model on the engineered features. Call the train_model tool.\",\n",
    "        llm_config=llm_config,\n",
    "        functions=[train_model]\n",
    "    )\n",
    "\n",
    "    evaluation_agent = ConversableAgent(\n",
    "        name=\"Model_Evaluation_Agent\",\n",
    "        system_message=\"Your task is to evaluate the trained model's performance on the test data. Call the evaluate_model tool and provide a summary.\",\n",
    "        llm_config=llm_config,\n",
    "        functions=[evaluate_model]\n",
    "    )\n",
    "\n",
    "# --- User Proxy Agent ---\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    is_termination_msg=lambda x: \"FINAL REPORT\" in x.get(\"content\", \"\"),\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    code_execution_config=False,\n",
    ")\n",
    "\n",
    "entry_agent.handoffs.set_after_work(AgentTarget(cleaning_agent))\n",
    "cleaning_agent.handoffs.set_after_work(AgentTarget(feature_agent))\n",
    "feature_agent.handoffs.set_after_work(AgentTarget(training_agent))\n",
    "training_agent.handoffs.set_after_work(AgentTarget(evaluation_agent))\n",
    "evaluation_agent.handoffs.set_after_work(RevertToUserTarget())\n",
    "\n",
    "\n",
    "\n",
    "# --- Define the Agent Flow ---\n",
    "agent_pattern = DefaultPattern(\n",
    "    initial_agent=entry_agent,\n",
    "    agents=[entry_agent, cleaning_agent, feature_agent, training_agent, evaluation_agent],\n",
    "    user_agent=user_proxy,\n",
    "    context_variables=shared_context,\n",
    ")\n",
    "\n",
    "\n",
    "# --- Function to run the pipeline ---\n",
    "def run_data_science_pipeline(file_path: str):\n",
    "    \"\"\"Initiates and runs the data science pipeline for a given data file.\"\"\"\n",
    "    print(f\"üöÄ Starting Data Science Pipeline for file: {file_path}...\\n\")\n",
    "    \n",
    "    chat_result = initiate_group_chat(\n",
    "        pattern=agent_pattern,\n",
    "        messages=f\"Start the data science pipeline. The data is located at: {file_path}\",\n",
    "        max_rounds=20,\n",
    "    )\n",
    "\n",
    "    print(\"\\nüèÅ Data Science Pipeline Finished.\\n\")\n",
    "    \n",
    "    # Print a summary from the last message or context\n",
    "    final_context = chat_result.final_context\n",
    "    if not final_context[\"has_error\"]:\n",
    "        print(\"‚úÖ Pipeline completed successfully!\\n\")\n",
    "        print(\"--- FINAL REPORT ---\")\n",
    "        \n",
    "        # Extracting details from the final message from the evaluation agent\n",
    "        last_message_content = chat_result.chat_history[-2].get('content', '')\n",
    "        summary = \"No summary found.\"\n",
    "        try:\n",
    "            # The tool call result is a string, so we need to parse it\n",
    "            result_json = json.loads(last_message_content)\n",
    "            summary = result_json.get('model_performance_summary', summary)\n",
    "            test_accuracy = result_json.get('test_accuracy', 0)\n",
    "            report = result_json.get('classification_report', {})\n",
    "            \n",
    "            print(f\"üìÑ Summary: {summary}\")\n",
    "            print(f\"üéØ Test Accuracy: {test_accuracy:.4f}\")\n",
    "            print(\"\\nüìä Classification Report:\")\n",
    "            print(json.dumps(report, indent=2))\n",
    "\n",
    "        except (json.JSONDecodeError, AttributeError, IndexError) as e:\n",
    "            print(f\"Could not parse final report from agent message. Error: {e}\")\n",
    "            print(\"Last message content:\", last_message_content)\n",
    "\n",
    "    else:\n",
    "        print(f\"‚ùå Pipeline failed at stage: {final_context['error_stage']}\")\n",
    "        print(f\"   Error: {final_context['error_message']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_data_science_pipeline(file_path=\"data/house_prices/train.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
